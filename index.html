<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="description"
          content="A short starter to build website for your paper.">
    <meta name="keywords" content="Paper, website">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>SSG MAKE 2026</title> <!-- the browser tab will display this text -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
      window.dataLayer = window.dataLayer || [];

function gtag() {
  dataLayer.push(arguments);
}

gtag('js', new Date());

gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/robot-svgrepo-com.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
  </head>
  <body>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">A Scene Graph Backed Approach to <br> Open Set Semantic Mapping</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://www.dfki.de/en/web/about-us/employee/person/magu02">Martin Günther</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="https://www.dfki.de/en/web/about-us/employee/person/igfe01">Felix Igelbrink</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="https://www.dfki.de/en/web/about-us/employee/person/osli01">Oscar Lima</a><sup>1*</sup>,</span>
                <br>
                <span class="author-block">
                  <a href="https://www.dfki.de/en/web/about-us/employee/person/leni01">Lennart Niecksch</a><sup>1,2*</sup>,</span>
                <span class="author-block">
                  <a href="https://www.dfki.de/en/web/about-us/employee/person/mare10">Marian Renz</a><sup>1,2*</sup>,</span>
                <br>
                <span class="author-block">
                  <a href="https://martin.atzmueller.net/">Martin Atzmueller</a><sup>2,1</sup></span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup>German Research Center for Artificial Intelligence (DFKI), Cooperative and Autonomous Systems (CAS), Hamburger Straße 24, Osnabrück, Germany.</span>
                <span class="author-block"><sup>2</sup>Osnabrück University, Semantic Information Systems Group, Wachsbleiche 27, Osnabrück, Germany.</span>
                <span class="author-block"><sup>*</sup>Equal contribution.</span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2602.03781"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2602.03781"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                  <!-- Video Link. -->
                  <span class="link-block">
                    <!-- TODO: replace with your own video YouTube link -->
                    <a href="https://youtu.be/ergZbnIya-c"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span>
                  <!-- Code Link. -->
                  <span class="link-block">
                    <!-- TODO: replace with your own github paper code link -->
                    <a href="https://github.com/DFKI-NI/TODO"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <!-- Dataset Link. -->
                  <!-- <span class="link-block">
                    TODO: if your paper has a dataset you can uncomment this block
                    <a href="https://github.com/todo"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                    <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                    </a> -->
                </div>

              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- <section class="hero teaser">
      <div class="container is-max-desktop">
      <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
      <source src="./static/videos/teaser.mp4"
      type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
      <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
      free-viewpoint
      portraits.
      </h2>
      </div>
      </div>
      </section> -->

      <section class="section">
        <div class="container is-max-desktop">
          <!-- Abstract. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Abstract</h2>
              <div class="content has-text-justified">
                <p>
                While Open Set Semantic Mapping and 3D Semantic Scene Graphs (3DSSGs) are established paradigms in robotic perception, deploying them effectively to support high-level reasoning in large-scale, real-world environments remains a significant challenge. Most existing approaches decouple perception from representation, treating the scene graph as a derivative layer generated post hoc. This limits both consistency and scalability. In contrast, we propose a mapping architecture where the 3DSSG serves as the foundational backend, acting as the primary knowledge representation for the entire mapping process.

                Our approach leverages prior work on incremental scene graph prediction to infer and update the graph structure in real-time as the environment is explored. This ensures that the map remains topologically consistent and computationally efficient, even during extended operations in large-scale settings. By maintaining an explicit, spatially grounded representation that supports both flat and hierarchical topologies, we bridge the gap between sub-symbolic raw sensor data and high-level symbolic reasoning. Consequently, this provides a stable, verifiable structure that knowledge-driven frameworks, ranging from knowledge graphs and ontologies to Large Language Models (LLMs), can directly exploit, enabling agents to operate with enhanced interpretability, trustworthiness, and alignment to human concepts.
                </p>
              </div>
            </div>
          </div>

          <!-- Paper video. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Video</h2>
              <div class="publication-video">
                <!-- TODO: After uploading your video to YouTube click on "share" then "Embed" then copy-paste <iframe ... </iframe> then replace sample video below -->
                <iframe width="560" height="315" src="https://www.youtube.com/embed/ergZbnIya-c?si=fxxTMDDyy3-3F5FK" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
              </div>
            </div>
          </div>
          <!--/ Paper video. -->
        </div>
      </section>

      <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <!--<pre><code>@InProceedings{guenther2026scenegraph,
  author    = {G{\"u}nther, Martin and Igelbrink, Felix and Lima, Oscar and Niecksch, Lennart and Renz, Marian and Atzmueller, Martin},
  title     = {A Scene Graph Backed Approach to Open Set Semantic Mapping},
  booktitle = {TODO},
  year      = {2026},
  pages     = {TODO},
  publisher = {TODO},
  doi       = {TODO},
}</code></pre>-->
          <pre><code>@misc{guenther2026scenegraph,
      title={A Scene Graph Backed Approach to Open Set Semantic Mapping},
      author={G{\"u}nther, Martin and Igelbrink, Felix and Lima, Oscar and Niecksch, Lennart and Renz, Marian and Atzmueller, Martin},
      year={2026},
      eprint={2602.03781},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2602.03781},
}</code></pre>
        </div>
      </section>

      <footer class="footer">
        <div class="container">
          <div class="content has-text-centered">
            <!-- <a class="icon-link"
              href="./static/files/TODO.pdf">
              <i class="fas fa-file-pdf"></i>
              </a> -->
              <a href="https://github.com/dfki-ni" class="external-link" disabled>
                <i class="fab fa-github"></i>
              </a>
          </div>
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <center><p>
                  This website (<a href="https://github.com/DFKI-NI/SSG-MAKE-2026/">source code</a>) was adapted from the popular <a
                                   href="https://nerfies.github.io">Nerfies</a> project page and is licensed under a <br><a rel="license"
                                                                                                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                                                                                                                            Commons Attribution-ShareAlike 4.0 International License</a>.
                  </p></center>
              </div>
            </div>
          </div>
        </div>
      </footer>

  </body>
</html>
